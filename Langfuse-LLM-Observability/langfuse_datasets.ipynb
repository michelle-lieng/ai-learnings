{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff719951",
   "metadata": {},
   "source": [
    "## Let's do some dataset runs in langfuse\n",
    "\n",
    "A dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset. \n",
    "\n",
    "1. First step make a Langfuse dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cd2e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cmfp9we0500g4ad070gio1s97', name='my-first-dataset', description='My first dataset', metadata={'date': '2025-18-09', 'type': 'benchmark', 'author': 'ML'}, project_id='cmfp5po0q05r3ad06e3l1053t', created_at=datetime.datetime(2025, 9, 18, 10, 33, 47, 237000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 18, 11, 19, 35, 967000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# MUST ALWAYS INITIALIZE LANGFUSE FIRST\n",
    "load_dotenv()\n",
    "langfuse = get_client()\n",
    "\n",
    "langfuse.create_dataset(\n",
    "    name=\"my-first-dataset\",\n",
    "    # optional description\n",
    "    description=\"My first dataset\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"ML\",\n",
    "        \"date\": \"2025-18-09\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43475f62",
   "metadata": {},
   "source": [
    "2. Create new dataset items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9226c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='4bf6de3c-a678-4704-b0a5-215706f68997', status=<DatasetStatus.ACTIVE: 'ACTIVE'>, input={'text': 'What is 2+1?'}, expected_output={'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 1 more.\\n                    2+1=3. âœ…\\n                    So the answer is 3.'}, metadata={'model': 'openai'}, source_trace_id=None, source_observation_id=None, dataset_id='cmfp9we0500g4ad070gio1s97', dataset_name='my-first-dataset', created_at=datetime.datetime(2025, 9, 18, 11, 19, 36, 301000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 18, 11, 19, 36, 301000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"my-first-dataset\",\n",
    "    # any python object or value, optional\n",
    "    input={\n",
    "        \"text\": \"What is 2+1?\"\n",
    "    },\n",
    "    # any python object or value, optional\n",
    "    expected_output={\n",
    "        \"text\": \"\"\"Letâ€™s do it carefully, digit by digit:\n",
    "                    Start with 2.\n",
    "                    Add 1 more.\n",
    "                    2+1=3. âœ…\n",
    "                    So the answer is 3.\"\"\"\n",
    "    },\n",
    "    # metadata, optional\n",
    "    metadata={\n",
    "        \"model\": \"openai\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04406d",
   "metadata": {},
   "source": [
    "Can also create synthetic datasets which I cover in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847755b",
   "metadata": {},
   "source": [
    "Create items from production data\n",
    "\n",
    "A common workflow is to select production traces where the application did not perform as expected. Then you let an expert add the expected output to test new versions of your application on the same data.\n",
    "\n",
    "AH HA MOMENT: So basically you can use this in prod or whenver you have some input that previously performed badly and you can put it in your dataset and the source doesn't really do anything except now you can click on it and it takes you back to the trace you got the example from so you can compare it.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"..\\assets\\langfuse\\langfuse-source.png\" alt=\"Langfuse Source\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f1269d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='87f865f8-ff85-41e3-9a85-b1958e61fb8a', status=<DatasetStatus.ACTIVE: 'ACTIVE'>, input={'text': 'hello world'}, expected_output={'text': 'hello world'}, metadata=None, source_trace_id='c5a2292d7a59688e2eec3be7e0145109', source_observation_id=None, dataset_id='cmfp9we0500g4ad070gio1s97', dataset_name='my-first-dataset', created_at=datetime.datetime(2025, 9, 18, 11, 19, 36, 624000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 18, 11, 19, 36, 624000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"my-first-dataset\",\n",
    "    input={ \"text\": \"hello world\" },\n",
    "    expected_output={ \"text\": \"hello world\" },\n",
    "    # optional: link to a trace\n",
    "    source_trace_id=\"c5a2292d7a59688e2eec3be7e0145109\",\n",
    "    # optional: link to a specific span, event, or generation\n",
    "    #source_observation_id=\"<observation_id>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafcffe",
   "metadata": {},
   "source": [
    "You can also edit/archive dataset items\n",
    "Archiving items will remove them from future experiment runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc82e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='9bbaa9c7-3ab3-4cad-b13b-cd7ea7edcfb3', status=<DatasetStatus.ARCHIVED: 'ARCHIVED'>, input='What is a dog?', expected_output=None, metadata=None, source_trace_id='c5a2292d7a59688e2eec3be7e0145109', source_observation_id=None, dataset_id='cmfp9we0500g4ad070gio1s97', dataset_name='my-first-dataset', created_at=datetime.datetime(2025, 9, 18, 10, 48, 55, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 18, 11, 26, 16, 961000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"my-first-dataset\",\n",
    "    id=\"9bbaa9c7-3ab3-4cad-b13b-cd7ea7edcfb3\",\n",
    "    # example: update status to \"ARCHIVED\"\n",
    "    status=\"ARCHIVED\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e59eb1",
   "metadata": {},
   "source": [
    "To edit them just use the same dataset item id as edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89cb21e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='9bbaa9c7-3ab3-4cad-b13b-cd7ea7edcfb3', status=<DatasetStatus.ARCHIVED: 'ARCHIVED'>, input='What is a cat?', expected_output=None, metadata=None, source_trace_id='c5a2292d7a59688e2eec3be7e0145109', source_observation_id=None, dataset_id='cmfp9we0500g4ad070gio1s97', dataset_name='my-first-dataset', created_at=datetime.datetime(2025, 9, 18, 10, 48, 55, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 18, 11, 27, 8, 321000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"my-first-dataset\",\n",
    "    id=\"9bbaa9c7-3ab3-4cad-b13b-cd7ea7edcfb3\",\n",
    "    input=\"What is a cat?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1d689",
   "metadata": {},
   "source": [
    "## Do remote dataset runs \n",
    "Once you created a dataset, you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\n",
    "\n",
    "They are called â€œRemote Dataset Runsâ€ because they can make use of â€œremoteâ€ or external logic and code.\n",
    "\n",
    "FIRST EXAMPLE NO LANGFUSE DATASET\n",
    "\n",
    "When running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b233d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Results: Hidden (2 items)\\nğŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nğŸ§ª Experiment: Geography Quiz\n",
      "ğŸ“‹ Run name: Geography Quiz - 2025-09-18T11:43:27.599485Z - Testing basic functionality\\n2 items\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from langfuse.openai import OpenAI\n",
    " \n",
    "# Initialize client\n",
    "langfuse = get_client()\n",
    " \n",
    "# Define your task function\n",
    "def my_task(*, item, **kwargs):\n",
    "    question = item[\"input\"]\n",
    "    response = OpenAI().chat.completions.create(\n",
    "        model=\"gpt-4.1\", messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    " \n",
    "    return response.choices[0].message.content\n",
    " \n",
    " \n",
    "# Run experiment on local data\n",
    "local_data = [\n",
    "    {\"input\": \"What is the capital of France?\"},\n",
    "    {\"input\": \"What is the capital of Germany?\"},\n",
    "]\n",
    " \n",
    "result = langfuse.run_experiment(\n",
    "    name=\"Geography Quiz\",\n",
    "    description=\"Testing basic functionality\",\n",
    "    data=local_data,\n",
    "    task=my_task,\n",
    ")\n",
    " \n",
    "# Use format method to display results\n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11266684",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"..\\assets\\langfuse\\langfuse-no-dataset-remote-run.png\" alt=\"Langfuse\" width=\"900\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed021a26",
   "metadata": {},
   "source": [
    "Now try Usage with Langfuse Datasets\n",
    "Run experiments directly on datasets stored in Langfuse for automatic tracing and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1215d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 0: id=87f865f8-ff85-41e3-9a85-b1958e61fb8a, input={'text': 'hello world'}, expected_output={'text': 'hello world'}, status=DatasetStatus.ARCHIVED\n",
      "Item 1: id=4bf6de3c-a678-4704-b0a5-215706f68997, input={'text': 'What is 2+1?'}, expected_output={'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 1 more.\\n                    2+1=3. âœ…\\n                    So the answer is 3.'}, status=DatasetStatus.ACTIVE\n",
      "Item 2: id=9bbaa9c7-3ab3-4cad-b13b-cd7ea7edcfb3, input=What is a cat?, expected_output=None, status=DatasetStatus.ARCHIVED\n",
      "Item 3: id=2deec1d3-d621-437a-ad04-1a01dd6351d0, input={'text': 'hello world'}, expected_output={'text': 'hello world'}, status=DatasetStatus.ACTIVE\n",
      "Item 4: id=443c3054-9777-48c0-831e-76da3cbf52d6, input={'text': 'What is 2+1?'}, expected_output={'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 1 more.\\n                    2+1=3. âœ…\\n                    So the answer is 3.'}, status=DatasetStatus.ACTIVE\n",
      "Item 5: id=26d0f54d-e2b1-4cd5-925a-df6866013473, input={'text': 'What is 2+2?'}, expected_output={'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 2 more.\\n                    2+2=4. âœ…\\n                    So the answer is 4.'}, status=DatasetStatus.ACTIVE\n"
     ]
    }
   ],
   "source": [
    "# VIEW ALL TRACES IN DATASET\n",
    "\n",
    "# Get your dataset object\n",
    "dataset = langfuse.get_dataset(\"my-first-dataset\")\n",
    "\n",
    "# Fetch all items\n",
    "items = dataset.items\n",
    "\n",
    "# Print each item's id and input\n",
    "for i, item in enumerate(items):\n",
    "    print(f\"Item {i}: id={item.id}, input={item.input}, expected_output={item.expected_output}, status={getattr(item, 'status', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bed54c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to create dataset run item: status_code: 404, body: {'message': 'Dataset item not found', 'error': 'LangfuseNotFoundError'}\n",
      "Failed to create dataset run item: status_code: 404, body: {'message': 'Dataset item not found', 'error': 'LangfuseNotFoundError'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Results: Hidden (6 items)\\nğŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nğŸ§ª Experiment: Production Model Test\n",
      "ğŸ“‹ Run name: Production Model Test - 2025-09-18T11:55:08.809831Z - Monthly evaluation of our production model\\n6 items\n"
     ]
    }
   ],
   "source": [
    "# Get dataset from Langfuse\n",
    "dataset = langfuse.get_dataset(\"my-first-dataset\")\n",
    "\n",
    "# Define your task function\n",
    "def prod_task(*, item, **kwargs):\n",
    "    # item.input could be a dict\n",
    "    if isinstance(item.input, dict):\n",
    "        question = item.input.get(\"text\", \"\")\n",
    "    # or item.input could be a string\n",
    "    else:\n",
    "        question = item.input\n",
    "    response = OpenAI().chat.completions.create(\n",
    "        model=\"gpt-4.1\", messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    " \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run experiment directly on the dataset\n",
    "result = dataset.run_experiment(\n",
    "    name=\"Production Model Test\",\n",
    "    description=\"Monthly evaluation of our production model\",\n",
    "    task=prod_task\n",
    ")\n",
    " \n",
    "# Use format method to display results\n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e53fb",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"..\\assets\\langfuse\\dataset-run-with-dataset.png\" alt=\"Langfuse\" width=\"900\"/>\n",
    "    <img src=\"..\\assets\\langfuse\\first-dataset-run.png\" alt=\"Langfuse\" width=\"900\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b055f",
   "metadata": {},
   "source": [
    "# Enhance your dataset runs with evaluators\n",
    "Evaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c88d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='fd80f2eb-4c23-4e12-a885-2f9b65d6874e' dataset_run_id='2be6100a-b272-4313-90c5-7f83c65dca1a' dataset_run_name='Multi-metric Evaluation - 2025-09-18T12:02:18.920890Z' dataset_item_id='26d0f54d-e2b1-4cd5-925a-df6866013473' trace_id='9d91cb22fe6fcb0afe948ef97d33b3e6' observation_id='1238b30038485665' created_at=datetime.datetime(2025, 9, 18, 12, 2, 34, 51000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 12, 2, 34, 51000, tzinfo=datetime.timezone.utc)\n",
      "id='2e6e5bcd-baf4-458b-a8b0-a41ee372f06e' dataset_run_id='2be6100a-b272-4313-90c5-7f83c65dca1a' dataset_run_name='Multi-metric Evaluation - 2025-09-18T12:02:18.920890Z' dataset_item_id='443c3054-9777-48c0-831e-76da3cbf52d6' trace_id='4c509c49f3f429a2994e00d1f1d8df73' observation_id='5b41419152cf9395' created_at=datetime.datetime(2025, 9, 18, 12, 2, 32, 604000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 12, 2, 32, 604000, tzinfo=datetime.timezone.utc)\n",
      "id='4d5932d8-e302-4c2c-975c-348908645f0f' dataset_run_id='2be6100a-b272-4313-90c5-7f83c65dca1a' dataset_run_name='Multi-metric Evaluation - 2025-09-18T12:02:18.920890Z' dataset_item_id='2deec1d3-d621-437a-ad04-1a01dd6351d0' trace_id='2a8a432d7e656af37f93cf266a99420d' observation_id='054ef11969a4694c' created_at=datetime.datetime(2025, 9, 18, 12, 2, 31, 160000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 12, 2, 31, 160000, tzinfo=datetime.timezone.utc)\n",
      "id='1676e158-cbbe-4c22-b6ed-b9c81780f990' dataset_run_id='2be6100a-b272-4313-90c5-7f83c65dca1a' dataset_run_name='Multi-metric Evaluation - 2025-09-18T12:02:18.920890Z' dataset_item_id='4bf6de3c-a678-4704-b0a5-215706f68997' trace_id='dcc09cfe8ccb87962fe058c30421a008' observation_id='ff5dcfdcee4de0db' created_at=datetime.datetime(2025, 9, 18, 12, 2, 25, 317000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 12, 2, 25, 317000, tzinfo=datetime.timezone.utc)\n",
      "id='9be82ff3-6cc8-4736-a25b-1a033ee0d51e' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='26d0f54d-e2b1-4cd5-925a-df6866013473' trace_id='fd41743a8f75dbe2a0de485100564c33' observation_id='923076ee2f6eb5c5' created_at=datetime.datetime(2025, 9, 18, 11, 55, 20, 996000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 20, 996000, tzinfo=datetime.timezone.utc)\n",
      "id='1bdc4d05-ec83-4420-8adf-4cd2c2a68bc0' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='443c3054-9777-48c0-831e-76da3cbf52d6' trace_id='6834fec8b3edf6a8f7ba76e5af3d5015' observation_id='e36704ac22973345' created_at=datetime.datetime(2025, 9, 18, 11, 55, 19, 658000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 19, 658000, tzinfo=datetime.timezone.utc)\n",
      "id='9ca6bee4-5281-4511-a280-43d9a7da7752' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='2deec1d3-d621-437a-ad04-1a01dd6351d0' trace_id='0c9b7b6a8271615812ac995a1bf8415d' observation_id='a7e2b371d67b7c9f' created_at=datetime.datetime(2025, 9, 18, 11, 55, 18, 349000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 18, 349000, tzinfo=datetime.timezone.utc)\n",
      "id='afc9426d-6f8f-4ffb-a067-3cb5519b875c' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='4bf6de3c-a678-4704-b0a5-215706f68997' trace_id='5d39416094d447a87fb11b87b2e27baa' observation_id='a79a71e2a689503d' created_at=datetime.datetime(2025, 9, 18, 11, 55, 12, 598000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 12, 598000, tzinfo=datetime.timezone.utc)\n",
      "id='0bf7c79d-2123-434d-a3e4-2f614abd7876' dataset_run_id='94c593d7-4325-48df-8913-84b87fa50d71' dataset_run_name='Production Model Test - 2025-09-18T11:47:51.222647Z' dataset_item_id='26d0f54d-e2b1-4cd5-925a-df6866013473' trace_id='92ce7063308faf1ffba1088645fefd09' observation_id='d1f8e90d00aad4ac' created_at=datetime.datetime(2025, 9, 18, 11, 48, 1, 72000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 48, 1, 72000, tzinfo=datetime.timezone.utc)\n",
      "id='87d1aaf3-62f7-435d-b831-2f77b47e13f4' dataset_run_id='94c593d7-4325-48df-8913-84b87fa50d71' dataset_run_name='Production Model Test - 2025-09-18T11:47:51.222647Z' dataset_item_id='443c3054-9777-48c0-831e-76da3cbf52d6' trace_id='7adb1bfb164ae8149a8788b4a400f569' observation_id='2f1f3b7f35e95d4b' created_at=datetime.datetime(2025, 9, 18, 11, 47, 58, 716000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 58, 716000, tzinfo=datetime.timezone.utc)\n",
      "id='3a794c3a-bbd9-4bb9-b5e6-0d5e6c6ba311' dataset_run_id='94c593d7-4325-48df-8913-84b87fa50d71' dataset_run_name='Production Model Test - 2025-09-18T11:47:51.222647Z' dataset_item_id='2deec1d3-d621-437a-ad04-1a01dd6351d0' trace_id='bff8d34a420e206dc561c670a4f65cab' observation_id='5ab4918e2db98f98' created_at=datetime.datetime(2025, 9, 18, 11, 47, 56, 911000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 56, 911000, tzinfo=datetime.timezone.utc)\n",
      "id='f88cc4ea-76a2-4ea7-bc24-7cd30c3e4fbb' dataset_run_id='94c593d7-4325-48df-8913-84b87fa50d71' dataset_run_name='Production Model Test - 2025-09-18T11:47:51.222647Z' dataset_item_id='4bf6de3c-a678-4704-b0a5-215706f68997' trace_id='e53a53bc801d362c1b92370aee0e3f41' observation_id='eef173c4c82bf621' created_at=datetime.datetime(2025, 9, 18, 11, 47, 55, 361000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 55, 361000, tzinfo=datetime.timezone.utc)\n",
      "id='dcc858a6-f134-4c81-a6e7-8cc8a41960f5' dataset_run_id='09104049-253f-4548-8c17-fc7e094f8f21' dataset_run_name='Production Model Test - 2025-09-18T11:47:04.300147Z' dataset_item_id='26d0f54d-e2b1-4cd5-925a-df6866013473' trace_id='32262081471d3b007a29b0f20ef1d7a6' observation_id='16b2882e62414d59' created_at=datetime.datetime(2025, 9, 18, 11, 47, 14, 652000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 14, 652000, tzinfo=datetime.timezone.utc)\n",
      "id='4eba493b-3d58-4893-bfc2-d1daf4085ca2' dataset_run_id='09104049-253f-4548-8c17-fc7e094f8f21' dataset_run_name='Production Model Test - 2025-09-18T11:47:04.300147Z' dataset_item_id='443c3054-9777-48c0-831e-76da3cbf52d6' trace_id='166065a962a0d66ab7de1aa264a94688' observation_id='bec5c3c4d209a6a4' created_at=datetime.datetime(2025, 9, 18, 11, 47, 12, 381000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 12, 381000, tzinfo=datetime.timezone.utc)\n",
      "id='080143e2-e51a-4204-a696-a2b73ce2eaf6' dataset_run_id='09104049-253f-4548-8c17-fc7e094f8f21' dataset_run_name='Production Model Test - 2025-09-18T11:47:04.300147Z' dataset_item_id='2deec1d3-d621-437a-ad04-1a01dd6351d0' trace_id='23a2fe33d94be99c26320829ab8aff82' observation_id='39b9b394b2df3d0a' created_at=datetime.datetime(2025, 9, 18, 11, 47, 11, 34000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 11, 34000, tzinfo=datetime.timezone.utc)\n",
      "id='1385d7e2-7bac-4079-9815-f97b231181b8' dataset_run_id='09104049-253f-4548-8c17-fc7e094f8f21' dataset_run_name='Production Model Test - 2025-09-18T11:47:04.300147Z' dataset_item_id='4bf6de3c-a678-4704-b0a5-215706f68997' trace_id='a3f8be611ec680e87bcbbfec51f646f3' observation_id='56da3897868e800e' created_at=datetime.datetime(2025, 9, 18, 11, 47, 7, 753000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 47, 7, 753000, tzinfo=datetime.timezone.utc)\n"
     ]
    }
   ],
   "source": [
    "runs_list = langfuse.api.datasets.get_runs(\n",
    "    dataset_name=\"my-first-dataset\", page=1, limit=100\n",
    ")\n",
    "\n",
    "for run_summary in runs_list.data:\n",
    "    run_full = langfuse.api.datasets.get_run(\n",
    "        dataset_name=\"my-first-dataset\", run_name=run_summary.name\n",
    "    )\n",
    "    for run_item in run_full.dataset_run_items:\n",
    "        print(run_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbda5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='9be82ff3-6cc8-4736-a25b-1a033ee0d51e' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='26d0f54d-e2b1-4cd5-925a-df6866013473' trace_id='fd41743a8f75dbe2a0de485100564c33' observation_id='923076ee2f6eb5c5' created_at=datetime.datetime(2025, 9, 18, 11, 55, 20, 996000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 20, 996000, tzinfo=datetime.timezone.utc)\n",
      "Input: {'text': 'What is 2+2?'}\n",
      "Expected Output: {'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 2 more.\\n                    2+2=4. âœ…\\n                    So the answer is 4.'}\n",
      "id='1bdc4d05-ec83-4420-8adf-4cd2c2a68bc0' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='443c3054-9777-48c0-831e-76da3cbf52d6' trace_id='6834fec8b3edf6a8f7ba76e5af3d5015' observation_id='e36704ac22973345' created_at=datetime.datetime(2025, 9, 18, 11, 55, 19, 658000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 19, 658000, tzinfo=datetime.timezone.utc)\n",
      "Input: {'text': 'What is 2+1?'}\n",
      "Expected Output: {'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 1 more.\\n                    2+1=3. âœ…\\n                    So the answer is 3.'}\n",
      "id='9ca6bee4-5281-4511-a280-43d9a7da7752' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='2deec1d3-d621-437a-ad04-1a01dd6351d0' trace_id='0c9b7b6a8271615812ac995a1bf8415d' observation_id='a7e2b371d67b7c9f' created_at=datetime.datetime(2025, 9, 18, 11, 55, 18, 349000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 18, 349000, tzinfo=datetime.timezone.utc)\n",
      "Input: {'text': 'hello world'}\n",
      "Expected Output: {'text': 'hello world'}\n",
      "id='afc9426d-6f8f-4ffb-a067-3cb5519b875c' dataset_run_id='dda46a69-a167-4853-aa24-7522ec531da6' dataset_run_name='Production Model Test - 2025-09-18T11:55:08.809831Z' dataset_item_id='4bf6de3c-a678-4704-b0a5-215706f68997' trace_id='5d39416094d447a87fb11b87b2e27baa' observation_id='a79a71e2a689503d' created_at=datetime.datetime(2025, 9, 18, 11, 55, 12, 598000, tzinfo=datetime.timezone.utc) updated_at=datetime.datetime(2025, 9, 18, 11, 55, 12, 598000, tzinfo=datetime.timezone.utc)\n",
      "Input: {'text': 'What is 2+1?'}\n",
      "Expected Output: {'text': 'Letâ€™s do it carefully, digit by digit:\\n                    Start with 2.\\n                    Add 1 more.\\n                    2+1=3. âœ…\\n                    So the answer is 3.'}\n"
     ]
    }
   ],
   "source": [
    "run_full = langfuse.api.datasets.get_run(\n",
    "    dataset_name=\"my-first-dataset\", run_name=\"Production Model Test - 2025-09-18T11:55:08.809831Z\"\n",
    ")\n",
    "\n",
    "for run_item in run_full.dataset_run_items:\n",
    "    print(run_item)\n",
    "    # Fetch the dataset item to get input and expected output\n",
    "    dataset_item = langfuse.api.dataset_items.get(run_item.dataset_item_id)\n",
    "    print(\"Input:\", dataset_item.input)\n",
    "    print(\"Expected Output:\", dataset_item.expected_output)\n",
    "    # Output is not directly stored on the DatasetItem; it is typically found in the trace linked via run_item.trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to create dataset run item: status_code: 404, body: {'message': 'Dataset item not found', 'error': 'LangfuseNotFoundError'}\n",
      "Evaluator accuracy_evaluator failed: 'dict' object has no attribute 'lower'\n",
      "Evaluator accuracy_evaluator failed: 'dict' object has no attribute 'lower'\n",
      "Failed to create dataset run item: status_code: 404, body: {'message': 'Dataset item not found', 'error': 'LangfuseNotFoundError'}\n",
      "Evaluator accuracy_evaluator failed: 'NoneType' object has no attribute 'lower'\n",
      "Evaluator accuracy_evaluator failed: 'dict' object has no attribute 'lower'\n",
      "Evaluator accuracy_evaluator failed: 'dict' object has no attribute 'lower'\n",
      "Evaluator accuracy_evaluator failed: 'dict' object has no attribute 'lower'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Results: Hidden (6 items)\\nğŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nğŸ§ª Experiment: Multi-metric Evaluation\n",
      "ğŸ“‹ Run name: Multi-metric Evaluation - 2025-09-18T12:02:18.920890Z\\n6 items\\nEvaluations:\\n  â€¢ response_length\\n\\nAverage Scores:\\n  â€¢ response_length: 206.833\\n\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Evaluation\n",
    " \n",
    "# Define evaluation functions\n",
    "def accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\n",
    "    if output.get(\"text\").lower() in expected_output.get(\"text\").lower() in output.lower():\n",
    "        return Evaluation(name=\"accuracy\", value=1.0, comment=\"Correct answer found\")\n",
    " \n",
    "    return Evaluation(name=\"accuracy\", value=0.0, comment=\"Incorrect answer\")\n",
    " \n",
    "def length_evaluator(*, input, output, **kwargs):\n",
    "    return Evaluation(name=\"response_length\", value=len(output), comment=f\"Response has {len(output)} characters\")\n",
    " \n",
    "# Use multiple evaluators\n",
    "result = dataset.run_experiment(\n",
    "    name=\"Multi-metric Evaluation\",\n",
    "    task=prod_task,\n",
    "    evaluators=[accuracy_evaluator, length_evaluator]\n",
    ")\n",
    " \n",
    "print(result.format())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
