{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e353a2b8",
   "metadata": {},
   "source": [
    "## Using llm as a judge to assess for accuracy\n",
    "\n",
    "Will follow methadology as per anthropic paper [here](https://www.anthropic.com/engineering/multi-agent-research-system):\n",
    "\"\"\"\n",
    " We used an LLM judge that evaluated each output against criteria in a rubric: factual accuracy (do claims match sources?), citation accuracy (do the cited sources match the claims?), completeness (are all requested aspects covered?), source quality (did it use primary sources over lower-quality secondary sources?), and tool efficiency (did it use the right tools a reasonable number of times?). We experimented with multiple judges to evaluate each component, but found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements. This method was especially effective when the eval test cases did have a clear answer, and we could use the LLM judge to simply check if the answer was correct (i.e. did it accurately list the pharma companies with the top 3 largest R&D budgets?).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7be9744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was the first president of the United States?\n",
      "The first president of the United States was George Washington, inaugurated in 1789.\n",
      "George Washington\n",
      "grounded\n",
      "Who was the first president of the United States?\n",
      "The first president of the United States was George Washington, inaugurated in 1789.\n",
      "Abraham Lincoln, who was inaugurated in 1861.\n",
      "ungrounded\n",
      "At what temperature does water boil at sea level?\n",
      "Water boils at 100°C at sea level under standard atmospheric pressure.\n",
      "100°C\n",
      "grounded\n",
      "At what temperature does water boil at sea level?\n",
      "Water boils at 100°C at sea level under standard atmospheric pressure.\n",
      "Water boils at 150°C at sea level.\n",
      "ungrounded\n",
      "What is 2 + 2?\n",
      "The sum of 2 and 2 is 4.\n",
      "2 plus 2 equals 4.\n",
      "grounded\n",
      "What is 2 + 2?\n",
      "The sum of 2 and 2 is 4.\n",
      "2 plus 2 equals 5.\n",
      "ungrounded\n",
      "Who are the main characters in Romeo and Juliet?\n",
      "In Shakespeare’s play Romeo and Juliet, the two main characters are Romeo Montague and Juliet Capulet.\n",
      "Romeo Montague and Juliet Capulet.\n",
      "grounded\n",
      "Who are the main characters in Romeo and Juliet?\n",
      "In Shakespeare’s play Romeo and Juliet, the two main characters are Romeo Montague and Juliet Capulet.\n",
      "The play’s main characters are Hamlet and Ophelia.\n",
      "ungrounded\n"
     ]
    }
   ],
   "source": [
    "# Simulated some data\n",
    "data = [\n",
    "    {\"question\": \"Who was the first president of the United States?\", \"context\": \"The first president of the United States was George Washington, inaugurated in 1789.\", \"response\": \"George Washington\", \"label\": \"grounded\"},\n",
    "    {\"question\": \"Who was the first president of the United States?\", \"context\": \"The first president of the United States was George Washington, inaugurated in 1789.\", \"response\": \"Abraham Lincoln, who was inaugurated in 1861.\", \"label\": \"ungrounded\"},\n",
    "    {\"question\": \"At what temperature does water boil at sea level?\", \"context\": \"Water boils at 100°C at sea level under standard atmospheric pressure.\", \"response\": \"100°C\", \"label\": \"grounded\"},\n",
    "    {\"question\": \"At what temperature does water boil at sea level?\", \"context\": \"Water boils at 100°C at sea level under standard atmospheric pressure.\", \"response\": \"Water boils at 150°C at sea level.\", \"label\": \"ungrounded\"},\n",
    "    {\"question\": \"What is 2 + 2?\", \"context\": \"The sum of 2 and 2 is 4.\", \"response\": \"2 plus 2 equals 4.\", \"label\": \"grounded\"},\n",
    "    {\"question\": \"What is 2 + 2?\", \"context\": \"The sum of 2 and 2 is 4.\", \"response\": \"2 plus 2 equals 5.\", \"label\": \"ungrounded\"},\n",
    "    {\"question\": \"Who are the main characters in Romeo and Juliet?\", \"context\": \"In Shakespeare’s play Romeo and Juliet, the two main characters are Romeo Montague and Juliet Capulet.\", \"response\": \"Romeo Montague and Juliet Capulet.\", \"label\": \"grounded\"},\n",
    "    {\"question\": \"Who are the main characters in Romeo and Juliet?\", \"context\": \"In Shakespeare’s play Romeo and Juliet, the two main characters are Romeo Montague and Juliet Capulet.\", \"response\": \"The play’s main characters are Hamlet and Ophelia.\", \"label\": \"ungrounded\"}\n",
    "]\n",
    "\n",
    "for x in data:\n",
    "    print(x.get(\"question\"))\n",
    "    print(x.get(\"context\"))\n",
    "    print(x.get(\"response\"))\n",
    "    print(x.get(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac91f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 1.0, 'pass': True}\n",
      "{'score': 0.0, 'pass': False}\n",
      "{'score': 1.0, 'pass': True}\n",
      "{'score': 0.0, 'pass': False}\n",
      "{'score': 1.0, 'pass': True}\n",
      "{'score': 0.0, 'pass': False}\n",
      "{'score': 1.0, 'pass': True}\n",
      "{'score': 0.0, 'pass': False}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "system_prompt = \"\"\"You are an impartial evaluator. \n",
    "Judge ONLY factual accuracy of the ANSWER against the provided CONTEXT. \n",
    "Use no outside knowledge. Return STRICT JSON per schema. \n",
    "No extra text. No chain-of-thought.\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "SCORING & DECISION:\n",
    "- factual_accuracy 0.0–1.0 (one decimal place).\n",
    "- pass = factual_accuracy >= {pass_threshold}\n",
    "\n",
    "OUTPUT JSON SCHEMA:\n",
    "{{\n",
    "  \"score\": 0.0,\n",
    "  \"pass\": false,\n",
    "}}\n",
    "\n",
    "RETURN ONLY JSON.\n",
    "\"\"\"\n",
    "\n",
    "def judge_factual(question, answer, sources, pass_threshold=0.8):\n",
    "    user_prompt = user_prompt_template.format(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        context=sources,\n",
    "        pass_threshold=pass_threshold,\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}]\n",
    "    )\n",
    "    out = resp.choices[0].message.content.strip()\n",
    "    return json.loads(out)\n",
    "\n",
    "for x in data:\n",
    "    print(judge_factual(question = x.get(\"question\"),\n",
    "                  answer = x.get(\"response\"),\n",
    "                  sources = x.get(\"context\")))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
