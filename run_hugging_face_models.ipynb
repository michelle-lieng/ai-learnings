{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0e5936",
   "metadata": {},
   "source": [
    "## How to Run Hugging Face Models\n",
    "\n",
    "What is Hugging Face?\n",
    "----------------------\n",
    "Hugging face is like GitHub for ML models: tons of open models (NLP, vision, audio, diffusion). You can download weights and run them locally or host them yourself. Each model repo has: weights (.bin, .safetensors, .gguf), config (config.json), tokenizer files, and training metadata. Hugging Face also builds libraries (transformers, datasets, diffusers) to run those models easily in Python.\n",
    "- Pros: Variety, full control, fine-tuning, offline use, cost-efficient at scale.\n",
    "- Cons: You must manage GPUs/infra and optimize for speed.\n",
    "\n",
    "HF Models VS. API Endpoints\n",
    "---------------------------\n",
    "Let's compare calling hugging face models instead of via API Endpoints(e.g. OpenAI, Anthropic, Gemini, etc.). Here you never see weights—just call an API and you pay $$ per token/requests.\n",
    "- Pros: Easy, reliable, no infra headaches.\n",
    "- Cons: Closed, $$$, limited models, can’t customize or fine-tune, no offline use.\n",
    "\n",
    "When to choose?\n",
    "---------------\n",
    "- Hugging Face → experimentation, control, cheap at scale, fine-tuning.\n",
    "- API → quick use, guaranteed uptime, best single model, less hassle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a61c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b264a41",
   "metadata": {},
   "source": [
    "### Self-Hosting Hugging Face (for maximum control and customization):\n",
    "Local Deployment: Run models locally using the Transformers library for testing and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600aae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Michelle\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face hosts thousands of models... (your article here) (your articles here) Click here for more information on how to get involved with Hugging Face. Click here to find out more about the site and to sign up for a free trial of the site. Visit www.HuggingFace.com.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"facebook/bart-large-cnn\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "text = \"Hugging Face hosts thousands of models... (your article here)\"\n",
    "\n",
    "inputs = tok(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "out = mdl.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d024c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face provides a platform with thousands of models for natural language processing (NLP) tasks, fostering collaboration and accessibility for developers and researchers.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Summarize this: Hugging Face hosts thousands of models...\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f004c45",
   "metadata": {},
   "source": [
    "### 1) Hugging Face Inference API (managed, like OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb369c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The word 'huggingface' contains three occurences of the letter\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(api_key=HUGGING_FACE_API_KEY)\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"How many 'G's in 'huggingface'?\"}],\n",
    "    max_tokens=16,\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f898f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's tackle the problem: **How many 'G's are in the word 'huggingface'?**\n",
      "\n",
      "### Understanding the Problem\n",
      "First, I need to understand what the question is asking. We're given the word \"huggingface,\" and we need to count how many times the letter 'G' (both uppercase and lowercase, though here it's all lowercase) appears in it.\n",
      "\n",
      "### Breaking Down the Word\n",
      "Let's write out the word and look at each letter one by one.\n",
      "\n",
      "The word is: h u g g i n g f a c e\n",
      "\n",
      "Let's index each letter for clarity:\n",
      "1. h\n",
      "2. u\n",
      "3. g\n",
      "4. g\n",
      "5. i\n",
      "6. n\n",
      "7. g\n",
      "8. f\n",
      "9. a\n",
      "10. c\n",
      "11. e\n",
      "\n",
      "Now, let's go through each position and see if the letter is 'g':\n",
      "\n",
      "1. h - not g\n",
      "2. u - not g\n",
      "3. g - yes (1st g)\n",
      "4. g - yes (2nd g)\n",
      "5. i - not g\n",
      "6. n - not g\n",
      "7. g - yes (3rd g)\n",
      "8. f - not g\n",
      "9. a - not g\n",
      "10. c - not g\n",
      "11. e - not g\n",
      "\n",
      "### Counting the 'G's\n",
      "From the above, we can see that 'g' appears at positions:\n",
      "- 3rd letter\n",
      "- 4th letter\n",
      "- 7th letter\n",
      "\n",
      "That's a total of 3 times.\n",
      "\n",
      "### Verifying\n",
      "Just to be sure, let's read the word again: \"huggingface.\"\n",
      "\n",
      "- h u g g i n g f a c e\n",
      "\n",
      "Counting the 'g's:\n",
      "- The first 'g' is the 3rd letter.\n",
      "- Then there's another 'g' right after it (4th letter).\n",
      "- Later, before the 'f', there's another 'g' (7th letter).\n",
      "\n",
      "No other 'g's are present.\n",
      "\n",
      "### Potential Pitfalls\n",
      "One might wonder if the uppercase 'G' is different from lowercase 'g', but in this word, all letters are lowercase, so that's not an issue here. Also, sometimes letters can be silent or combined sounds, but since we're counting letters, not sounds, that doesn't affect our count.\n",
      "\n",
      "### Final Answer\n",
      "After carefully examining each letter in \"huggingface,\" we find that the letter 'g' appears **3 times**. \n",
      "\n",
      "**Answer:** There are 3 'G's in 'huggingface'.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=HUGGING_FACE_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How many 'G's in 'huggingface'?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(model=\"deepseek-ai/DeepSeek-V3-0324\", messages=messages)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06142bb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A**_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      2\u001b[39m client = OpenAI(api_key=\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m r = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow many \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mG\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms in \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhuggingface\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(r.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A**_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "r = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"How many 'G's in 'huggingface'?\"}],\n",
    "    max_tokens=16,\n",
    ")\n",
    "print(r.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b8a18",
   "metadata": {},
   "source": [
    "### 2) Hugging Face Transformers (local weights + tokenizer)\n",
    "\n",
    "Downloads weights & tokenizer and runs on your machine. Using Mistral-7B-Instruct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c9360fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "     ---------------------------------------- 0.0/50.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.3/50.7 MB 6.1 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 2.6/50.7 MB 6.6 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 3.9/50.7 MB 6.5 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 5.5/50.7 MB 6.6 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 6.6/50.7 MB 6.4 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 7.9/50.7 MB 6.2 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 8.9/50.7 MB 6.1 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 10.2/50.7 MB 6.0 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 11.3/50.7 MB 6.0 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 12.3/50.7 MB 5.9 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 13.6/50.7 MB 5.9 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 14.9/50.7 MB 5.9 MB/s eta 0:00:07\n",
      "     ------------ --------------------------- 16.0/50.7 MB 5.9 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 17.3/50.7 MB 5.9 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 18.6/50.7 MB 5.8 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 19.7/50.7 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 21.0/50.7 MB 5.8 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 22.0/50.7 MB 5.8 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 23.3/50.7 MB 5.8 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 24.6/50.7 MB 5.8 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 25.7/50.7 MB 5.8 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 27.0/50.7 MB 5.8 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 28.3/50.7 MB 5.8 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 29.9/50.7 MB 5.9 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 31.2/50.7 MB 5.9 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 32.5/50.7 MB 5.9 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 34.1/50.7 MB 5.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 35.4/50.7 MB 6.0 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 36.7/50.7 MB 6.0 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 38.3/50.7 MB 6.0 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 39.6/50.7 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 40.9/50.7 MB 6.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 42.5/50.7 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 43.8/50.7 MB 6.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 45.1/50.7 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 46.4/50.7 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 48.0/50.7 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 49.0/50.7 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  50.3/50.7 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  50.6/50.7 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 50.7/50.7 MB 6.0 MB/s  0:00:08\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python) (2.3.0)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp311-cp311-win_amd64.whl size=6775319 sha256=f997f1506c2660a5a940766c03a8d43de0c7974b0aa73b80ee78726dc53e9a0a\n",
      "  Stored in directory: c:\\users\\michelle\\appdata\\local\\pip\\cache\\wheels\\d8\\5b\\e5\\a7d4b5765da347d314e8155197440c9995a962f8e4a5f52b23\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "\n",
      "   ---------------------------------------- 0/2 [diskcache]\n",
      "   -------------------- ------------------- 1/2 [llama-cpp-python]\n",
      "   -------------------- ------------------- 1/2 [llama-cpp-python]\n",
      "   -------------------- ------------------- 1/2 [llama-cpp-python]\n",
      "   -------------------- ------------------- 1/2 [llama-cpp-python]\n",
      "   -------------------- ------------------- 1/2 [llama-cpp-python]\n",
      "   -------------------- ------------------- 1/2 [llama-cpp-python]\n",
      "   ---------------------------------------- 2/2 [llama-cpp-python]\n",
      "\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e6dbc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from Llama-3.2-1B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-1B-Instruct-GGU...\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q4_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 762.81 MiB (5.18 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = Llama 3.2 1B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 66 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   445.50 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   762.81 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      "..........\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\n",
      "llama_kv_cache_unified: size =   64.00 MiB (  2048 cells,  16 layers,  1/1 seqs), K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 1176\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   254.50 MiB\n",
      "llama_context: graph nodes  = 566\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Llama 3.2 1B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'llama.block_count': '16', 'general.basename': 'Llama-3.2', 'general.finetune': 'Instruct', 'general.size_label': '1B', 'general.license': 'llama3.2', 'llama.context_length': '131072', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.entries_count': '112', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '64', 'llama.attention.value_length': '64', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "llama_perf_context_print:        load time =     200.60 ms\n",
      "llama_perf_context_print: prompt eval time =     200.47 ms /    12 tokens (   16.71 ms per token,    59.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5313.56 ms /   125 runs   (   42.51 ms per token,    23.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    5675.21 ms /   137 tokens\n",
      "llama_perf_context_print:    graphs reused =        120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any 'G's in 'growing'? \n",
      "\n",
      "## Step 1: Count the number of 'G's in 'huggingface'\n",
      "There is 1 'G' in 'huggingface'.\n",
      "\n",
      "## Step 2: Count the number of 'G's in 'growing'\n",
      "There are 2 'G's in 'growing'.\n",
      "\n",
      "## Step 3: Calculate the total number of 'G's\n",
      "Total number of 'G's = 1 (from 'huggingface') + 2 (from 'growing') = 3.\n",
      "\n",
      "The final answer is: $\\boxed{3}$\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# 1. Load the quantized model (GGUF format)\n",
    "llm = Llama(\n",
    "    model_path=\"Llama-3.2-1B-Instruct-Q4_K_M.gguf\",  # point to your downloaded model file\n",
    "    n_ctx=2048                                            # max context length (# of tokens it can \"remember\")\n",
    ")\n",
    "\n",
    "# 2. Define your prompt\n",
    "prompt = \"How many 'G's in 'huggingface'?\"\n",
    "\n",
    "# 3. Run inference\n",
    "out = llm(prompt, max_tokens=250)\n",
    "\n",
    "# 4. Print the model's answer\n",
    "print(out[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea52acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.42\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Collecting filelock (from transformers>=4.42)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.42)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.42) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.42) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.42) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.42)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.42) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.42)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.42)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.42) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (4.14.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers>=4.42) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.42) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.42) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.42) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.42) (2025.4.26)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 7.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.3 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.3 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.5/11.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 6.3 MB/s  0:00:01\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Downloading torch-2.8.0-cp311-cp311-win_amd64.whl (241.4 MB)\n",
      "   ---------------------------------------- 0.0/241.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/241.4 MB 6.7 MB/s eta 0:00:36\n",
      "   ---------------------------------------- 2.6/241.4 MB 6.6 MB/s eta 0:00:37\n",
      "    --------------------------------------- 4.2/241.4 MB 6.6 MB/s eta 0:00:36\n",
      "    --------------------------------------- 5.2/241.4 MB 6.2 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 6.6/241.4 MB 6.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 7.6/241.4 MB 5.9 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 8.7/241.4 MB 5.8 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 10.0/241.4 MB 5.8 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 11.0/241.4 MB 5.8 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 12.3/241.4 MB 5.8 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 13.4/241.4 MB 5.7 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 14.7/241.4 MB 5.8 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 16.0/241.4 MB 5.8 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 17.0/241.4 MB 5.8 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 18.4/241.4 MB 5.8 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 19.7/241.4 MB 5.7 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 21.2/241.4 MB 5.8 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 22.5/241.4 MB 5.8 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 23.9/241.4 MB 5.9 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 25.2/241.4 MB 5.9 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 26.7/241.4 MB 5.9 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 28.0/241.4 MB 5.9 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 29.4/241.4 MB 6.0 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 30.9/241.4 MB 6.0 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 32.2/241.4 MB 6.0 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 33.6/241.4 MB 6.0 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 35.1/241.4 MB 6.1 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 36.4/241.4 MB 6.1 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 37.7/241.4 MB 6.1 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 39.3/241.4 MB 6.1 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 40.4/241.4 MB 6.1 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 41.9/241.4 MB 6.1 MB/s eta 0:00:33\n",
      "   ------- -------------------------------- 43.3/241.4 MB 6.1 MB/s eta 0:00:33\n",
      "   ------- -------------------------------- 44.8/241.4 MB 6.2 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 45.9/241.4 MB 6.1 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 46.4/241.4 MB 6.1 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 46.9/241.4 MB 6.0 MB/s eta 0:00:33\n",
      "   ------- -------------------------------- 47.2/241.4 MB 5.9 MB/s eta 0:00:34\n",
      "   ------- -------------------------------- 47.4/241.4 MB 5.8 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 48.8/241.4 MB 5.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 49.8/241.4 MB 5.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 51.1/241.4 MB 5.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 52.4/241.4 MB 5.7 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 53.5/241.4 MB 5.7 MB/s eta 0:00:34\n",
      "   --------- ------------------------------ 54.5/241.4 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 55.8/241.4 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 56.9/241.4 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 58.2/241.4 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 59.5/241.4 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 60.3/241.4 MB 5.6 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 61.6/241.4 MB 5.7 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 63.2/241.4 MB 5.7 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 64.5/241.4 MB 5.7 MB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 65.8/241.4 MB 5.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 67.1/241.4 MB 5.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 68.7/241.4 MB 5.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 70.0/241.4 MB 5.8 MB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 71.3/241.4 MB 5.8 MB/s eta 0:00:30\n",
      "   ------------ --------------------------- 72.6/241.4 MB 5.8 MB/s eta 0:00:30\n",
      "   ------------ --------------------------- 74.2/241.4 MB 5.8 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 75.5/241.4 MB 5.8 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 76.8/241.4 MB 5.8 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 78.4/241.4 MB 5.8 MB/s eta 0:00:29\n",
      "   ------------- -------------------------- 79.4/241.4 MB 5.8 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 80.7/241.4 MB 5.8 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 81.8/241.4 MB 5.8 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 83.1/241.4 MB 5.8 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 84.4/241.4 MB 5.8 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 85.5/241.4 MB 5.8 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 86.8/241.4 MB 5.8 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 87.8/241.4 MB 5.8 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 89.1/241.4 MB 5.8 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 90.4/241.4 MB 5.8 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 91.0/241.4 MB 5.8 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 92.0/241.4 MB 5.7 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 93.1/241.4 MB 5.7 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 94.4/241.4 MB 5.7 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 95.4/241.4 MB 5.7 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 96.7/241.4 MB 5.7 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 98.3/241.4 MB 5.8 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 99.6/241.4 MB 5.8 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 100.9/241.4 MB 5.8 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 102.5/241.4 MB 5.8 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 103.8/241.4 MB 5.8 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 105.1/241.4 MB 5.8 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 106.7/241.4 MB 5.8 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 108.0/241.4 MB 5.8 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 109.3/241.4 MB 5.8 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 110.6/241.4 MB 5.8 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 111.9/241.4 MB 5.8 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 113.0/241.4 MB 5.8 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 114.0/241.4 MB 5.8 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 115.1/241.4 MB 5.8 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 116.4/241.4 MB 5.8 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 117.4/241.4 MB 5.8 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 118.5/241.4 MB 5.8 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 119.3/241.4 MB 5.8 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 120.1/241.4 MB 5.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 120.8/241.4 MB 5.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 121.9/241.4 MB 5.7 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 123.5/241.4 MB 5.7 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 124.8/241.4 MB 5.7 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 126.1/241.4 MB 5.7 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 127.7/241.4 MB 5.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 129.0/241.4 MB 5.8 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 130.5/241.4 MB 5.8 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 131.6/241.4 MB 5.8 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 133.2/241.4 MB 5.8 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 134.5/241.4 MB 5.8 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 136.1/241.4 MB 5.8 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 137.4/241.4 MB 5.8 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 138.7/241.4 MB 5.8 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 140.2/241.4 MB 5.8 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 141.6/241.4 MB 5.8 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 142.9/241.4 MB 5.8 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 144.4/241.4 MB 5.8 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 145.8/241.4 MB 5.8 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 147.1/241.4 MB 5.8 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 148.4/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 149.9/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 150.7/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 151.3/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 152.3/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 153.4/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 154.4/241.4 MB 5.8 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 155.5/241.4 MB 5.8 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 156.5/241.4 MB 5.8 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 157.3/241.4 MB 5.8 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 158.3/241.4 MB 5.8 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 159.9/241.4 MB 5.8 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 161.2/241.4 MB 5.8 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 162.5/241.4 MB 5.8 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 163.8/241.4 MB 5.8 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 165.2/241.4 MB 5.8 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 166.5/241.4 MB 5.8 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 167.8/241.4 MB 5.8 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 169.3/241.4 MB 5.8 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 170.7/241.4 MB 5.8 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 172.0/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 173.5/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 174.9/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 175.9/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 176.9/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 178.0/241.4 MB 5.8 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 178.5/241.4 MB 5.8 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 178.8/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 179.8/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 180.9/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 181.7/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 182.7/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 183.8/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 184.8/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 185.9/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 187.2/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 188.7/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 190.1/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 191.4/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 192.9/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 194.2/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 195.6/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.1/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 198.4/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 199.8/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 201.1/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 202.6/241.4 MB 5.7 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 203.9/241.4 MB 5.7 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 205.0/241.4 MB 5.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 205.8/241.4 MB 5.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 207.4/241.4 MB 5.7 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 208.7/241.4 MB 5.7 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 210.0/241.4 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 211.6/241.4 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 212.9/241.4 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 214.2/241.4 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 215.7/241.4 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 216.8/241.4 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 217.6/241.4 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 218.6/241.4 MB 5.7 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 219.7/241.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 220.7/241.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 222.0/241.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 223.1/241.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 224.4/241.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 225.4/241.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 226.5/241.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 227.8/241.4 MB 5.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 229.4/241.4 MB 5.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 230.7/241.4 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 232.0/241.4 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 233.6/241.4 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 234.4/241.4 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  235.7/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  237.0/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.3/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  239.9/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.4/241.4 MB 5.6 MB/s  0:00:42\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl (276 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 5.4 MB/s  0:00:00\n",
      "Installing collected packages: mpmath, sympy, safetensors, regex, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   ----------------------------------------  0/11 [mpmath]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   --- ------------------------------------  1/11 [sympy]\n",
      "   ------- --------------------------------  2/11 [safetensors]\n",
      "   ---------- -----------------------------  3/11 [regex]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   -------------- -------------------------  4/11 [networkx]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   ------------------ ---------------------  5/11 [fsspec]\n",
      "   --------------------- ------------------  6/11 [filelock]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ------------------------- --------------  7/11 [torch]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   ----------------------------- ----------  8/11 [huggingface-hub]\n",
      "   -------------------------------- -------  9/11 [tokenizers]\n",
      "   -------------------------------- -------  9/11 [tokenizers]\n",
      "   -------------------------------- -------  9/11 [tokenizers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ------------------------------------ --- 10/11 [transformers]\n",
      "   ---------------------------------------- 11/11 [transformers]\n",
      "\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.7.0 huggingface-hub-0.34.4 mpmath-1.3.0 networkx-3.5 regex-2025.7.34 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.2\n"
     ]
    }
   ],
   "source": [
    "# CPU-only quick start\n",
    "!pip install \"transformers>=4.42\" torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451a207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many 'G's in 'huggingface'? \n",
      "<|assistant|>\n",
      "The 'G' in 'huggingface' is a capital 'G' that is not a part of the actual word.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "model.eval()  # CPU\n",
    "\n",
    "msgs = [{\"role\":\"user\",\"content\":\"How many 'G's in 'huggingface'?\"}]\n",
    "prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tok(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=200, temperature=0.2)\n",
    "\n",
    "print(tok.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
